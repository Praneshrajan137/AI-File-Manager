# INTELLIGENCE LAYER CONTEXT (Custom LLM Integration)

## ğŸ¯ PURPOSE
This layer provides **"Chat with your File System"** functionality using a local LLM and vector database. It runs as a **background service** to avoid blocking the main UI thread.

## ğŸ—ï¸ ARCHITECTURE

### Responsibilities
1. **File Indexing**: Chunk and embed file content
2. **Vector Storage**: Store embeddings in LanceDB
3. **Retrieval**: Semantic search over file content
4. **Query Processing**: Interface with local LLM (Ollama)
5. **Context Management**: RAG pipeline for relevant context retrieval

### Directory Structure
```
src/llm/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ IndexingService.ts      # File content chunking & embedding
â”‚   â”œâ”€â”€ VectorStore.ts          # LanceDB wrapper
â”‚   â”œâ”€â”€ RetrievalService.ts     # RAG pipeline
â”‚   â”œâ”€â”€ LLMInterface.ts         # Ollama client
â”‚   â””â”€â”€ ContextManager.ts       # Context window optimization
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ EmbeddingModel.ts       # Embedding generation
â”‚   â””â”€â”€ PromptTemplates.ts      # System prompts for LLM
â””â”€â”€ CLAUDE.md                   # This file
```

## ğŸ” PRIVACY-FIRST DESIGN

**CRITICAL**: All processing happens LOCALLY. No cloud APIs.

- âœ… LanceDB runs in-process (no external server)
- âœ… Ollama runs locally (models stored on disk)
- âœ… File content never leaves machine
- âœ… Embeddings stored locally
```typescript
// âœ… Correct - Local LLM
import Ollama from 'ollama';

const ollama = new Ollama({ host: 'http://localhost:11434' });

// âŒ Wrong - Cloud API (violates privacy requirement)
import OpenAI from 'openai';
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
```

## ğŸ“Š RAG PIPELINE ARCHITECTURE
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. FILE WATCHER EVENT                         â”‚
â”‚     (New file created/modified)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. CHUNKING SERVICE                           â”‚
â”‚     - Split text into 500-token chunks         â”‚
â”‚     - 50-token overlap for context continuity  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. EMBEDDING MODEL                            â”‚
â”‚     - Generate vectors (384 dimensions)        â”‚
â”‚     - Model: all-MiniLM-L6-v2 (local)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. VECTOR STORE (LanceDB)                     â”‚
â”‚     - Store: chunk_text, embedding, metadata   â”‚
â”‚     - Index: IVF-PQ for fast similarity search â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Query Time:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. USER QUERY                                 â”‚
â”‚     "What are the main functions in auth.ts?"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. RETRIEVAL SERVICE                          â”‚
â”‚     - Embed query                              â”‚
â”‚     - Similarity search (top 10 chunks)        â”‚
â”‚     - Re-rank by relevance                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. CONTEXT COMPRESSION                        â”‚
â”‚     - If tokens > 4000: summarize chunks       â”‚
â”‚     - Preserve most relevant information       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. LLM PROMPT CONSTRUCTION                    â”‚
â”‚     System: "You are a code assistant..."      â”‚
â”‚     Context: [Retrieved chunks]                â”‚
â”‚     Query: [User question]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  9. OLLAMA LOCAL LLM                           â”‚
â”‚     Model: llama3.2 (3B parameters)            â”‚
â”‚     Streaming response to UI                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§© KEY COMPONENTS IMPLEMENTATION

### 1. Chunking Service
```typescript
// services/IndexingService.ts

interface ChunkResult {
  chunks: TextChunk[];
  metadata: FileMetadata;
}

interface TextChunk {
  text: string;
  startChar: number;
  endChar: number;
  chunkIndex: number;
}

export class IndexingService {
  private readonly CHUNK_SIZE = 500;  // tokens
  private readonly OVERLAP = 50;      // tokens
  
  /**
   * Chunk file content with overlap for context continuity.
   * Strategy: Sliding window with overlap to prevent losing context at boundaries.
   */
  async chunkFile(filePath: string): Promise<ChunkResult> {
    const content = await fs.readFile(filePath, 'utf-8');
    const tokens = this.tokenize(content);
    
    const chunks: TextChunk[] = [];
    let startIdx = 0;
    let chunkIndex = 0;
    
    while (startIdx < tokens.length) {
      const endIdx = Math.min(startIdx + this.CHUNK_SIZE, tokens.length);
      const chunkTokens = tokens.slice(startIdx, endIdx);
      
      chunks.push({
        text: this.detokenize(chunkTokens),
        startChar: this.getCharIndex(tokens, startIdx),
        endChar: this.getCharIndex(tokens, endIdx),
        chunkIndex: chunkIndex++,
      });
      
      // Move forward by CHUNK_SIZE - OVERLAP
      startIdx += this.CHUNK_SIZE - this.OVERLAP;
    }
    
    return {
      chunks,
      metadata: {
        filePath,
        totalChunks: chunks.length,
        indexedAt: Date.now(),
      },
    };
  }
  
  private tokenize(text: string): string[] {
    // Simple whitespace tokenization
    // In production, use proper tokenizer like tiktoken
    return text.split(/\s+/);
  }
}
```

### 2. Vector Store (LanceDB Wrapper)
```typescript
// services/VectorStore.ts
import lancedb from 'vectordb';

interface VectorRecord {
  id: string;
  chunk_text: string;
  embedding: number[];
  file_path: string;
  chunk_index: number;
  indexed_at: number;
}

export class VectorStore {
  private db: lancedb.Connection;
  private table: lancedb.Table;
  
  async initialize(dbPath: string): Promise<void> {
    this.db = await lancedb.connect(dbPath);
    
    // Create table if not exists
    try {
      this.table = await this.db.openTable('file_chunks');
    } catch {
      this.table = await this.db.createTable('file_chunks', [
        {
          id: 'example',
          chunk_text: '',
          embedding: new Array(384).fill(0),
          file_path: '',
          chunk_index: 0,
          indexed_at: Date.now(),
        },
      ]);
    }
  }
  
  /**
   * Add file chunks to vector store.
   */
  async addChunks(
    chunks: TextChunk[],
    embeddings: number[][],
    filePath: string
  ): Promise<void> {
    const records: VectorRecord[] = chunks.map((chunk, i) => ({
      id: `${filePath}:${chunk.chunkIndex}`,
      chunk_text: chunk.text,
      embedding: embeddings[i],
      file_path: filePath,
      chunk_index: chunk.chunkIndex,
      indexed_at: Date.now(),
    }));
    
    await this.table.add(records);
  }
  
  /**
   * Semantic search using vector similarity.
   * Returns top K most relevant chunks.
   */
  async search(
    queryEmbedding: number[],
    topK: number = 10
  ): Promise<VectorRecord[]> {
    return await this.table
      .search(queryEmbedding)
      .limit(topK)
      .execute();
  }
  
  /**
   * Delete all chunks for a file (e.g., when file is deleted).
   * Security: Manual escaping is used as LanceDB's filter() method requires string syntax.
   * Note: This is not true parameterized queries - it's defense-in-depth escaping.
   * Future: Migrate to object-based delete when LanceDB supports it.
   */
  async deleteFile(filePath: string): Promise<void> {
    // Escape single quotes to prevent injection (manual escaping pattern)
    // WARNING: This is NOT parameterized queries - it's string-based escaping
    const escapedPath = filePath.replace(/'/g, "''");
    
    await this.table
      .filter(`file_path = '${escapedPath}'`)
      .delete()
      .execute();
    
    // TODO: Replace with true parameterized pattern when LanceDB supports:
    // await this.table.delete({ file_path: filePath }).execute();
  }
}
```

### 3. Retrieval Service (RAG Core)
```typescript
// services/RetrievalService.ts

export class RetrievalService {
  private vectorStore: VectorStore;
  private embeddingModel: EmbeddingModel;
  private contextManager: ContextManager;
  
  /**
   * Retrieve relevant context for user query.
   * Implements: Retrieve â†’ Rerank â†’ Compress â†’ Return
   */
  async retrieveContext(query: string): Promise<RetrievalResult> {
    // 1. Embed query
    const queryEmbedding = await this.embeddingModel.embed(query);
    
    // 2. Similarity search (retrieve top 10)
    const candidates = await this.vectorStore.search(queryEmbedding, 10);
    
    // 3. Re-rank by relevance (optional: use cross-encoder)
    const reranked = this.rerank(query, candidates);
    
    // 4. Check total token count
    const totalTokens = this.countTokens(reranked);
    
    // 5. Compress if needed (context window management)
    let finalContext: string;
    if (totalTokens > 4000) {
      finalContext = await this.contextManager.compress(reranked);
    } else {
      finalContext = reranked.map(r => r.chunk_text).join('\n\n');
    }
    
    return {
      context: finalContext,
      sources: reranked.map(r => r.file_path),
      tokenCount: this.countTokens(finalContext),
    };
  }
  
  /**
   * Simple re-ranking using BM25 (keyword relevance).
   * Production: Use cross-encoder model for semantic reranking.
   */
  private rerank(query: string, candidates: VectorRecord[]): VectorRecord[] {
    // Calculate BM25 scores
    const scores = candidates.map(candidate => ({
      candidate,
      score: this.bm25Score(query, candidate.chunk_text),
    }));
    
    // Sort by score descending
    scores.sort((a, b) => b.score - a.score);
    
    return scores.map(s => s.candidate);
  }
  
  private bm25Score(query: string, document: string): number {
    // Simplified BM25 implementation
    // Production: Use proper BM25 library
    const queryTerms = query.toLowerCase().split(/\s+/);
    const docTerms = document.toLowerCase().split(/\s+/);
    
    let score = 0;
    for (const term of queryTerms) {
      const termFreq = docTerms.filter(t => t === term).length;
      score += termFreq > 0 ? Math.log(1 + termFreq) : 0;
    }
    
    return score;
  }
}
```

### 4. Context Manager (Compression)
```typescript
// services/ContextManager.ts

export class ContextManager {
  private llmInterface: LLMInterface;
  
  /**
   * Compress context when it exceeds token limit.
   * Strategy: Use smaller local model to summarize chunks.
   */
  async compress(chunks: VectorRecord[]): Promise<string> {
    const summaries: string[] = [];
    
    // Summarize each chunk individually
    for (const chunk of chunks) {
      const summary = await this.llmInterface.summarize(chunk.chunk_text, {
        maxTokens: 100,
        model: 'llama3.2:1b',  // Use smaller model for summarization
      });
      
      summaries.push(`[${chunk.file_path}]: ${summary}`);
    }
    
    return summaries.join('\n\n');
  }
  
  /**
   * Estimate token count (rough approximation).
   * Production: Use tiktoken library for accurate counting.
   */
  countTokens(text: string): number {
    // Rough estimate: 1 token â‰ˆ 4 characters
    return Math.ceil(text.length / 4);
  }
}
```

### 5. LLM Interface (Ollama Client)
```typescript
// services/LLMInterface.ts
import Ollama from 'ollama';

export class LLMInterface {
  private ollama: Ollama;
  
  constructor() {
    this.ollama = new Ollama({
      host: 'http://localhost:11434',
    });
  }
  
  /**
   * Query LLM with retrieved context.
   * Implements streaming for better UX.
   */
  async *query(
    userQuery: string,
    context: string,
    systemPrompt?: string
  ): AsyncGenerator<string> {
    const prompt = this.constructPrompt(userQuery, context, systemPrompt);
    
    const stream = await this.ollama.chat({
      model: 'llama3.2',
      messages: [
        { role: 'system', content: systemPrompt || this.defaultSystemPrompt() },
        { role: 'user', content: prompt },
      ],
      stream: true,
    });
    
    for await (const chunk of stream) {
      yield chunk.message.content;
    }
  }
  
  /**
   * Summarize text (for context compression).
   */
  async summarize(text: string, options: { maxTokens: number; model: string }): Promise<string> {
    const response = await this.ollama.chat({
      model: options.model,
      messages: [
        {
          role: 'system',
          content: 'Summarize the following text concisely, preserving key information.',
        },
        { role: 'user', content: text },
      ],
      options: {
        num_predict: options.maxTokens,
      },
    });
    
    return response.message.content;
  }
  
  private constructPrompt(query: string, context: string, systemPrompt?: string): string {
    return `Based on the following context from your file system:

${context}

Answer this question: ${query}

If the context doesn't contain enough information to answer, say so explicitly.`;
  }
  
  private defaultSystemPrompt(): string {
    return `You are a helpful assistant that answers questions about the user's files and code. 

Key behaviors:
1. Base your answers ONLY on the provided context
2. If context is insufficient, say "I don't have enough information in the indexed files"
3. Cite specific file names when relevant
4. Be concise but thorough
5. For code questions, explain logic and purpose`;
  }
}
```

## âš¡ BACKGROUND INDEXING STRATEGY

### Async Processing to Avoid Blocking UI
```typescript
// main.ts (Main Process)
import { FileWatcher } from './services/FileWatcher';
import { IndexingService } from '../llm/services/IndexingService';

const fileWatcher = new FileWatcher();
const indexingService = new IndexingService();

fileWatcher.on('fileChanged', async (filePath: string) => {
  // Don't block - queue for background processing
  queueIndexing(filePath);
});

const indexingQueue: string[] = [];
let isIndexing = false;

async function queueIndexing(filePath: string): Promise<void> {
  indexingQueue.push(filePath);
  
  if (!isIndexing) {
    processIndexingQueue();
  }
}

async function processIndexingQueue(): Promise<void> {
  isIndexing = true;
  
  while (indexingQueue.length > 0) {
    const filePath = indexingQueue.shift()!;
    
    try {
      await indexingService.indexFile(filePath);
    } catch (error) {
      console.error(`Failed to index ${filePath}:`, error);
    }
  }
  
  isIndexing = false;
}
```

## ğŸ§ª TESTING LLM LAYER

### Mock Ollama for Tests
```typescript
// tests/unit/LLMInterface.test.ts
jest.mock('ollama', () => ({
  Ollama: jest.fn().mockImplementation(() => ({
    chat: jest.fn().mockResolvedValue({
      message: { content: 'Mocked response' },
    }),
  })),
}));

describe('LLMInterface', () => {
  it('should construct prompt with context', async () => {
    const llm = new LLMInterface();
    const context = 'File auth.ts contains login function';
    const query = 'How does authentication work?';
    
    // Spy on Ollama.chat to verify prompt construction
    const chatSpy = jest.spyOn(llm['ollama'], 'chat');
    
    await llm.query(query, context);
    
    expect(chatSpy).toHaveBeenCalledWith(
      expect.objectContaining({
        messages: expect.arrayContaining([
          expect.objectContaining({
            content: expect.stringContaining(context),
          }),
        ]),
      })
    );
  });
});
```

## ğŸš¨ ANTI-PATTERNS (LLM Layer Specific)

1. **âŒ Blocking Main Thread**: Indexing large files synchronously
2. **âŒ No Error Handling**: Not catching Ollama connection failures
3. **âŒ Cloud Dependencies**: Using OpenAI API (violates privacy)
4. **âŒ No Token Limits**: Sending 10,000+ token contexts to LLM
5. **âŒ Ignoring File Types**: Trying to index binary files as text

## ğŸ“š REFERENCES

- `docs/ARCHITECTURE.md` - Integration with Main/Renderer processes
- LanceDB Docs: https://lancedb.github.io/lancedb/
- Ollama API: https://github.com/ollama/ollama/blob/main/docs/api.md

---

**CONTEXT SCOPE**: Intelligence Layer development ONLY
**LAST UPDATED**: December 4, 2025
